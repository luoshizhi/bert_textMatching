{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "flags = tf.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "processors = {\n",
    "    \"cola\": \"ColaProcessor\",\n",
    "    \"mnli\": \"MnliProcessor\",\n",
    "    \"mrpc\": \"MrpcProcessor\",\n",
    "    \"xnli\": \"XnliProcessor\",\n",
    "    \"sim\": SimProcessor,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS():\n",
    "    do_lower_case = True\n",
    "    init_checkpoint = \"chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "    bert_config_file = \"chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "    output_dir = \"tmp/sim_model\"\n",
    "    data_dir = \"data\" \n",
    "    task_name = \"sim\" \n",
    "    vocab_file = \"chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "    do_train = \"true\"\n",
    "    do_eval = \"true\"\n",
    "    max_seq_length = 50\n",
    "    train_batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查给的参数有没有问题\n",
    "  # 如果 model是cased model，不能设置--do_lower_case=True`，这样fine-tuning 才能匹配 pre-training model\n",
    "tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\n",
    "                                              FLAGS.init_checkpoint)\n",
    "# # 得到bert_config.__dict__,以字典形式读取config文件, 就能直接用bert_config.xxx的形式调用\n",
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "# 新建文件夹\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n",
      "<tokenization.FullTokenizer object at 0x11fbd88d0>\n"
     ]
    }
   ],
   "source": [
    "task_name = FLAGS.task_name.lower()\n",
    "processor = processors[task_name]()\n",
    "label_list = processor.get_labels()\n",
    "print(label_list)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile(FLAGS.bert_config_file, \"r\") as reader:\n",
    "      text = reader.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_probs_dropout_prob': 0.1, 'directionality': 'bidi', 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'pooler_fc_size': 768, 'pooler_num_attention_heads': 12, 'pooler_num_fc_layers': 3, 'pooler_size_per_head': 128, 'pooler_type': 'first_token_transform', 'type_vocab_size': 2, 'vocab_size': 21128}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_size', 'hidden_size', 'num_hidden_layers', 'num_attention_heads', 'hidden_act', 'intermediate_size', 'hidden_dropout_prob', 'attention_probs_dropout_prob', 'max_position_embeddings', 'type_vocab_size', 'initializer_range', 'directionality', 'pooler_fc_size', 'pooler_num_attention_heads', 'pooler_num_fc_layers', 'pooler_size_per_head', 'pooler_type'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "print(json.loads(text))\n",
    "modeling.BertConfig.from_dict(json.loads(text)).__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BertConfig.to_dict of <modeling.BertConfig object at 0x11f9d7cf8>>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six.iteritems(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
    "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
    "\n",
    "  # The casing has to be passed in by the user and there is no explicit check\n",
    "  # as to whether it matches the checkpoint. The casing information probably\n",
    "  # should have been stored in the bert_config.json file, but it's not, so\n",
    "  # we have to heuristically detect it to validate.\n",
    "\n",
    "  if not init_checkpoint:\n",
    "    return\n",
    "\n",
    "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
    "  if m is None:\n",
    "    return\n",
    "\n",
    "  model_name = m.group(1)\n",
    "\n",
    "  lower_models = [\n",
    "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
    "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
    "  ]\n",
    "\n",
    "  cased_models = [\n",
    "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
    "      \"multi_cased_L-12_H-768_A-12\"\n",
    "  ]\n",
    "\n",
    "  is_bad_config = False\n",
    "  if model_name in lower_models and not do_lower_case:\n",
    "    is_bad_config = True\n",
    "    actual_flag = \"False\"\n",
    "    case_name = \"lowercased\"\n",
    "    opposite_flag = \"True\"\n",
    "\n",
    "  if model_name in cased_models and do_lower_case:\n",
    "    is_bad_config = True\n",
    "    actual_flag = \"True\"\n",
    "    case_name = \"cased\"\n",
    "    opposite_flag = \"False\"\n",
    "\n",
    "  if is_bad_config:\n",
    "    raise ValueError(\n",
    "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
    "        \"However, `%s` seems to be a %s model, so you \"\n",
    "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
    "        \"how the model was pre-training. If this error is wrong, please \"\n",
    "        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
    "                                          model_name, case_name, opposite_flag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aa():\n",
    "    pass\n",
    "dd = aa()\n",
    "dd.__dict__['g']='a'\n",
    "dd.g #输出 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = aa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.__dict__['g']='a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "  def get_train_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @classmethod\n",
    "  def _read_tsv(cls, input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "      lines = []\n",
    "      for line in reader:\n",
    "        lines.append(line)\n",
    "      return lines\n",
    "\n",
    "class SimProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the Sim task\"\"\"\n",
    "\n",
    "  # read csv\n",
    "  # def get_train_examples(self, data_dir):\n",
    "  #   file_path = os.path.join(data_dir, 'train.csv')\n",
    "  #   train_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "  #   train_data = []\n",
    "  #   for index, train in enumerate(train_df.values):\n",
    "  #       guid = 'train-%d' % index\n",
    "  #       text_a = tokenization.convert_to_unicode(str(train[0]))\n",
    "  #       # text_b = tokenization.convert_to_unicode(str(train[1]))\n",
    "  #       label = str(train[1])\n",
    "  #       train_data.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "  #   return train_data\n",
    "\n",
    "  # read txt\n",
    "  def get_train_examples(self, data_dir):\n",
    "    file_path = os.path.join(data_dir, 'train.txt')\n",
    "    f = open(file_path, 'r')\n",
    "    train_data = []\n",
    "    index = 0\n",
    "    for line in f.readlines():\n",
    "        guid = 'train-%d' % index\n",
    "        line = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        text_a = tokenization.convert_to_unicode(str(line[0]))\n",
    "        text_b = tokenization.convert_to_unicode(str(line[1]))\n",
    "        label = str(line[2])\n",
    "        train_data.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        index += 1\n",
    "    return train_data\n",
    "\n",
    "  # csv\n",
    "  # def get_dev_examples(self, data_dir):\n",
    "  #   file_path = os.path.join(data_dir, 'dev.csv')\n",
    "  #   dev_df = pd.read_csv(file_path, encoding='utf-8')\n",
    "  #   dev_data = []\n",
    "  #   for index, dev in enumerate(dev_df.values):\n",
    "  #       guid = 'dev-%d' % index\n",
    "  #       text_a = tokenization.convert_to_unicode(str(dev[0]))\n",
    "  #       # text_b = tokenization.convert_to_unicode(str(dev[1]))\n",
    "  #       label = str(dev[1])\n",
    "  #       dev_data.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "  #   return dev_data\n",
    "\n",
    "  def get_dev_examples(self, data_dir):\n",
    "    file_path = os.path.join(data_dir, 'dev.txt')\n",
    "    f = open(file_path, 'r')\n",
    "    dev_data = []\n",
    "    index = 0\n",
    "    for line in f.readlines():\n",
    "        guid = 'dev-%d' % index\n",
    "        line = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        text_a = tokenization.convert_to_unicode(str(line[0]))\n",
    "        text_b = tokenization.convert_to_unicode(str(line[1]))\n",
    "        label = str(line[2])\n",
    "        dev_data.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        index += 1\n",
    "    return dev_data\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "    file_path = os.path.join(data_dir, 'test.txt')\n",
    "    f = open(file_path, 'r')\n",
    "    test_data = []\n",
    "    index = 0\n",
    "    for line in f.readlines():\n",
    "        guid = 'test-%d' % index\n",
    "        line = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "        text_a = tokenization.convert_to_unicode(str(line[0]))\n",
    "        text_b = tokenization.convert_to_unicode(str(line[1]))\n",
    "        label = str(line[2])\n",
    "        test_data.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        index += 1\n",
    "    return test_data\n",
    "\n",
    "  def get_labels(self):\n",
    "    return ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
